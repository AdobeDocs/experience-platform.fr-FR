---
title: Techniques de transformation des fonctionnalit√©s
description: D√©couvrez les techniques essentielles de pr√©traitement telles que la transformation, le codage et la mise √† l‚Äô√©chelle des fonctionnalit√©s des donn√©es, qui pr√©parent les donn√©es √† la formation aux mod√®les statistiques. Il couvre l‚Äôimportance de g√©rer les valeurs manquantes et de convertir les donn√©es cat√©gorielles afin d‚Äôam√©liorer les performances et la pr√©cision du mod√®le.
role: Developer
source-git-commit: b248e8f8420b617a117d36aabad615e5bbf66b58
workflow-type: tm+mt
source-wordcount: '3437'
ht-degree: 9%

---

# Techniques de transformation des fonctionnalit√©s

Les transformations sont des √©tapes essentielles de pr√©traitement qui convertissent ou dimensionnent les donn√©es dans un format adapt√© √† la formation et √† l‚Äôanalyse des mod√®les, assurant ainsi des performances optimales et une pr√©cision optimale. Ce document sert de ressource de syntaxe compl√©mentaire, fournissant des d√©tails d√©taill√©s sur les principales techniques de transformation des fonctionnalit√©s pour le pr√©traitement des donn√©es.

Les mod√®les d‚Äôapprentissage automatique ne peuvent pas traiter directement des valeurs de cha√Æne ou des valeurs nulles, ce qui rend le pr√©traitement des donn√©es essentiel. Ce guide explique comment utiliser diverses transformations pour imputer les valeurs manquantes, convertir les donn√©es cat√©goriques en formats num√©riques et appliquer des techniques de mise √† l‚Äô√©chelle des fonctionnalit√©s telles que le codage √† chaud et la vectorisation. Ces m√©thodes permettent aux mod√®les d‚Äôinterpr√©ter et d‚Äôapprendre efficacement des donn√©es, ce qui am√©liore finalement leurs performances.

## Transformation automatique des fonctionnalit√©s {#automatic-transformations}

Si vous choisissez d‚Äôignorer la clause `TRANSFORM` dans votre commande `CREATE MODEL`, la transformation des fonctionnalit√©s se produit automatiquement. Le pr√©traitement automatique des donn√©es comprend des transformations de fonctions standard et de remplacement nulles (bas√©es sur le type de donn√©es). Les colonnes num√©riques et de texte sont automatiquement imput√©es, puis des transformations de fonctionnalit√©s sont effectu√©es afin de s‚Äôassurer que les donn√©es sont dans un format appropri√© pour la formation des mod√®les d‚Äôapprentissage automatique. Ce processus comprend l‚Äôimputation des donn√©es manquante et des transformations cat√©goriques, num√©riques et bool√©ennes.

>[!IMPORTANT]
>
>La transformation des fonctionnalit√©s utilis√©e au moment de la formation sera √©galement utilis√©e pour la transformation des fonctionnalit√©s au moment de la pr√©diction et de l‚Äô√©valuation.

Les tableaux suivants expliquent comment diff√©rents types de donn√©es sont trait√©s lorsque la clause `TRANSFORM` est omise lors de la commande `CREATE MODEL`.

### Remplacement nul {#automatic-null-replacement}

| Type de donn√©es | Remplacement nul |
|-----------------|-----------------------------------------------------|
| Num√©rique | Les valeurs nulles sont remplac√©es par la valeur moyenne de la colonne. |
| Cat√©gorielle | Les valeurs nulles sont remplac√©es par le mot-cl√© `ml_unknown`. |
| Bool√©en | Les valeurs null sont remplac√©es par une valeur `FALSE`. |
| Date et heure | Il s‚Äôagit d‚Äôun champ continu. |
| Imbriqu√©/STRUCT | Le remplacement d√©pend du type de donn√©es du noeud terminal. |

### Transformation des fonctionnalit√©s {#automatic-feature-transformation}

| Type de donn√©es | Transformation des fonctionnalit√©s |
|-----------------|-----------------------------------------------------|
| Num√©rique | NOT REQUIRED : car ce type de donn√©es est compris par les algorithmes d‚Äôapprentissage automatique. |
| Cha√Æne | L‚Äôindexation des cha√Ænes se produit. |
| Bool√©en | L‚Äôindexation des cha√Ænes se produit. |
| Date et heure | Aucune op√©ration ne se produit. |
| STRUCTION | La valeur est √©tendue √† son noeud terminal. La transformation se produit en fonction du type de donn√©es du noeud terminal. |

**exemple**

```sql
CREATE model modelname options(model_type='logistic_reg', label='rating') AS SELECT * FROM movie_rating;
```

## Conversion manuelle des fonctionnalit√©s {#manual-transformations}

Pour d√©finir le pr√©traitement des donn√©es personnalis√© dans votre instruction `CREATE MODEL`, utilisez la clause `TRANSFORM` en combinaison avec n‚Äôimporte quel nombre de fonctions de transformation disponibles. Ces fonctions de pr√©traitement manuel peuvent √©galement √™tre utilis√©es en dehors de la clause `TRANSFORM`. Toutes les transformations mentionn√©es dans la section [transformateur ci-dessous](#available-transformations) peuvent √™tre utilis√©es pour pr√©traiter les donn√©es manuellement.

### Caract√©ristiques cl√©s

Voici les principales caract√©ristiques de la transformation des fonctionnalit√©s √† prendre en compte lorsque vous d√©finissez vos fonctions de pr√©traitement :

- **Syntaxe** : `TRANSFORM(functionName(colName, parameters) <aliasNAME>)`
   - Le nom de l&#39;alias est obligatoire dans la syntaxe. Vous devez indiquer un nom d‚Äôalias pour que la requ√™te √©choue.

- **Parameters** : les param√®tres sont des arguments de positionnement. Cela signifie que chaque param√®tre ne peut prendre que certaines valeurs. Pour plus d‚Äôinformations sur la fonction qui utilise cet argument, reportez-vous √† la documentation appropri√©e.

- **Cha√Ænage des transformateurs** : la sortie d‚Äôun transformateur peut devenir l‚Äôentr√©e d‚Äôun autre transformateur.

- **Utilisation des fonctionnalit√©s** : la derni√®re transformation des fonctionnalit√©s est utilis√©e comme une fonctionnalit√© du mod√®le d‚Äôapprentissage automatique.

**Exemple**

```sql
CREATE MODEL modelname 
TRANSFORM(
  string_imputer(language, 'adding_null') AS imp_language, 
  numeric_imputer(users_count, 'mode') AS imp_users_count, 
  string_indexer(imp_language) AS si_lang,  
  vector_assembler(array(imp_users_count, si_lang, watch_minutes)) AS features
)  
OPTIONS(MODEL_TYPE='logistic_reg', LABEL='rating') 
AS SELECT * FROM df;
```

## Transformations disponibles {#available-transformations}

Il existe 19 transformations disponibles. Ces transformations sont divis√©es en [transformations g√©n√©rales](#general-transformations), [transformations num√©riques](#numeric-transformations), [transformations cat√©gorielles](#categorical-transformations) et [transformations textuelles](#textual-transformations).

### Transformations g√©n√©rales {#general-transformations}

Lisez cette section pour plus de d√©tails sur les transformateurs utilis√©s pour un large √©ventail de types de donn√©es. Ces informations sont essentielles si vous devez appliquer des transformations qui ne sont pas sp√©cifiques aux donn√©es cat√©gorielles ou textuelles.

>[!NOTE]
>
>Le type de donn√©es d‚Äôentr√©e fait r√©f√©rence √† la colonne sur laquelle l‚Äôimputation est appliqu√©e. Le type de donn√©es de sortie fait r√©f√©rence √† la colonne qui est produite en tant que sortie une fois la transformation prise en compte.

#### Imprimeur num√©rique {#numeric-imputer}

Le transformateur **Numeric imputer** compl√®te les valeurs manquantes dans un jeu de donn√©es. Cette option utilise la moyenne, la m√©diane ou le mode des colonnes dans lesquelles se trouvent les valeurs manquantes. Les colonnes d‚Äôentr√©e doivent √™tre `DoubleType` ou `FloatType`. Vous trouverez plus d‚Äôinformations et d‚Äôexemples dans la [documentation de l‚Äôalgorithme Spark](https://spark.apache.org/docs/2.2.0/ml-features.html#imputer).

>[!NOTE]
>
>Toutes les valeurs null dans les colonnes d‚Äôentr√©e sont trait√©es comme manquantes, et donc √©galement imput√©es.

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : num√©rique
- Type de donn√©es de sortie : num√©rique

**D√©finition**

```sql
transformer(numeric_imputer(hour, 'mean') hour_imputed)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
| -------- | ------------ | ----- | -------- | -------- |
| `STRATEGY` | Une strat√©gie d&#39;imputation. Les valeurs disponibles sont : [`mean`, `median`, `mode`]. | Cha√Æne | mean | facultatif |

{style="table-layout:auto"}

**Exemple avant imputation**

| identifiant | hour |
|---|---|
| 0 | 18,0 |
| 1 | null |
| 2 | 8,0 |

**Exemple apr√®s imputation (avec strat√©gie moyenne)**

| identifiant | hour |
|---|---|
| 0 | 18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

#### Entier de cha√Æne {#string-imputer}

Le transformateur **cha√Æne imputer** compl√®te les valeurs manquantes dans un jeu de donn√©es √† l‚Äôaide d‚Äôune cha√Æne fournie par l‚Äôutilisateur en tant qu‚Äôargument de fonction. Les colonnes d‚Äôentr√©e et de sortie doivent √™tre de type de donn√©es `string`.

>[!NOTE]
>
>Toutes les valeurs null dans les colonnes d‚Äôentr√©e sont trait√©es comme manquantes et sont remplac√©es par la cha√Æne sp√©cifi√©e.

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : cha√Æne
- Type de donn√©es de sortie : cha√Æne

**D√©finition**

```sql
transform(string_imputer(name, 'unknown_name') as name_imputed)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | La valeur qui remplace null. | Cha√Æne | ml_unknown | facultatif |

{style="table-layout:auto"}

**Exemple avant imputation**

| identifiant | nom |
|---|---|
| 0 | John |
| 1 | null |
| 2 | Alice |

**Exemple apr√®s imputation (utilisation de &#39;ml_unknown&#39; comme remplacement)**

| identifiant | nom |
|---|---|
| 0 | John |
| 1 | ml_unknown |
| 2 | Alice |

#### Indice bool√©en {#imputer}

Le transformateur **** de l‚Äôattribut bool√©en compl√®te les valeurs manquantes dans un jeu de donn√©es pour une colonne bool√©enne. Les colonnes d‚Äôentr√©e et de sortie doivent √™tre de type `Boolean`.

>[!NOTE]
>
>Toutes les valeurs null dans les colonnes d‚Äôentr√©e sont trait√©es comme manquantes et sont remplac√©es par la valeur bool√©enne sp√©cifi√©e.

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : bool√©en
- Type de donn√©es de sortie : bool√©en

**D√©finition**

```sql
transform(boolean_imputer(name, true) as name_imputed)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
| -------- | ------------ | ----- | -------- | -------- |
| `NULL_REPLACEMENT` | Indice bool√©en. Valeurs autoris√©es : [`true`, `false`]. | Bool√©en | False | facultatif |

**Exemple avant imputation**

| identifiant | indicateur |
|---|---|
| 0 | vrai |
| 1 | null |
| 2 | False |

**Exemple apr√®s imputation (utilisation de &#39;true&#39; comme remplacement)**

| identifiant | indicateur |
|---|---|
| 0 | vrai |
| 1 | vrai |
| 2 | False |

#### Assembleur vectoriel {#vector-assembler}

Le transformateur `VectorAssembler` combine une liste sp√©cifi√©e de colonnes d‚Äôentr√©e dans une seule colonne vectorielle, ce qui facilite la gestion de plusieurs fonctionnalit√©s dans les mod√®les d‚Äôapprentissage automatique. Cela s‚Äôav√®re particuli√®rement utile pour fusionner des fonctionnalit√©s brutes et celles g√©n√©r√©es par diff√©rents transformateurs de fonctionnalit√©s en un seul vecteur de fonctionnalit√©s unifi√©. `VectorAssembler` accepte les colonnes d‚Äôentr√©e de types num√©rique, bool√©en et vectoriel. Dans chaque ligne, les valeurs des colonnes de saisie sont concat√©n√©es dans un vecteur dans l‚Äôordre indiqu√©.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#vectorassembler) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : `array[string]` (noms de colonne avec des valeurs num√©riques/de tableau [num√©riques])
- Type de donn√©es de sortie : `Vector[double]`

**D√©finition**

```sql
transform(vector_assembler(id, hour, mobile, userFeatures) as features)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
| -------- | ------------ | ----- | -------- | -------- |
| NA | Aucun param√®tre suppl√©mentaire n‚Äôest requis pour ce transformateur. | NA | NA | NA |

{style="table-layout:auto"}

**Exemple avant transformation**

| identifiant | hour | mobile | userFeatures | clicked |
|---|-------|--------|------------------|---------|
| 0 | 18 | 1.0 | [0.0, 10.0, 0.5] | 1.0 |

{style="table-layout:auto"}

**Exemple apr√®s transformation**

| identifiant | hour | mobile | userFeatures | clicked | fonctionnalit√©s |
|---|------|--------|------------------|---------|-------------------------------|
| 0 | 18 | 1.0 | [0.0, 10.0, 0.5] | 1.0 | [18.0, 1.0, 0.0, 10.0, 0.5] |

{style="table-layout:auto"}

### Transformations num√©riques {#numeric-transformations}

Lisez cette section pour en savoir plus sur les transformateurs disponibles pour le traitement et la mise √† l‚Äô√©chelle des donn√©es num√©riques. Ces transformateurs sont n√©cessaires pour g√©rer et optimiser les fonctionnalit√©s num√©riques de vos jeux de donn√©es.

#### Binarizer {#binarizer}

Le transformateur `Binarizer` convertit les fonctions num√©riques en fonctions binaires (0/1) par le biais d‚Äôun processus appel√© binarisation. Les valeurs de fonction sup√©rieures au seuil sp√©cifi√© sont converties en 1.0, tandis que les valeurs √©gales ou inf√©rieures au seuil sont converties en 0.0. `Binarizer` prend en charge les types `Vector` et `Double` pour la colonne d‚Äôentr√©e.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#binarizer). -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : colonne num√©rique
- Type de donn√©es de sortie : num√©rique

**D√©finition**

```sql
transform(numeric_imputer(rating, 'mode') rating_imp, binarizer(rating_imp) rating_binarizer)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|------------|----------------------------------------------------------------------------------------------------------|----------|----------|----------|
| `THRESHOLD` | Param pour le seuil utilis√© pour binariser les fonctionnalit√©s continues. Les fonctionnalit√©s sup√©rieures au seuil sont binaires √† 1.0, tandis que les fonctionnalit√©s √©gales ou inf√©rieures au seuil sont binaires √† 0.0. | int/double | 0,0 | facultatif |

{style="table-layout:auto"}

**Exemple d‚Äôentr√©e avant la binarisation**

| identifiant | note |
|---|---------|
| 0 | -18,0 |
| 1 | 13,0 |
| 2 | 8,0 |

**Exemple de sortie apr√®s la binarisation (seuil par d√©faut de 0.0)**

| identifiant | note |
|---|---------|
| 0 | 0,0 |
| 1 | 1.0 |
| 2 | 1.0 |

**D√©finition avec seuil personnalis√©**

```sql
transform(numeric_imputer(age, 'mode') age_imp, binarizer(age_imp, 14.0) age_binarizer)
```

**Exemple de sortie apr√®s la binarisation (avec un seuil de 14.0)**

| identifiant | age |
|---|-------|
| 0 | 0,0 |
| 1 | 0,0 |
| 2 | 1.0 |

#### Bucketizer {#bucketizer}

Le transformateur `Bucketizer` convertit une colonne de fonctionnalit√©s continues en une colonne de compartiments de fonctionnalit√©s, en fonction des seuils sp√©cifi√©s par l‚Äôutilisateur. Ce processus est utile pour segmenter les donn√©es continues en classes ou compartiments discrets. `Bucketizer` n√©cessite un param√®tre `splits` qui d√©finit les limites des compartiments.

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : colonne num√©rique
- Type de donn√©es de sortie : num√©rique (valeurs binaires)

**D√©finition**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|----------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|----------|----------|
| `splits` | Param√®tre permettant de mapper les fonctionnalit√©s continues en compartiments. Avec `n+1` divisions, il existe `n` compartiments. Les s√©parations doivent √™tre dans un ordre strictement croissant et la plage (x,y) est utilis√©e pour chaque compartiment, √† l‚Äôexception de la derni√®re, qui inclut y. | array(double) | S/O | facultatif |

{style="table-layout:auto"}

**Exemples de divisions**

- Tableau(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)
- Tableau(0.0, 1.0, 2.0)

Les s√©parations doivent couvrir toute la gamme des valeurs Double ; dans le cas contraire, les valeurs en dehors des s√©parations sp√©cifi√©es seront trait√©es comme des erreurs.

**Exemple de transformation**

Cet exemple utilise une colonne de fonctionnalit√©s continues (`course_duration`), les classe en fonction du `splits` fourni, puis assemble les compartiments r√©sultants avec d‚Äôautres fonctionnalit√©s.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MinMaxScaler {#minmaxscaler}

Le transformateur `MinMaxScaler` redimensionne chaque fonctionnalit√© d‚Äôun jeu de donn√©es de lignes vectorielles sur une plage sp√©cifi√©e, g√©n√©ralement [0, 1]. Cela garantit que toutes les fonctionnalit√©s contribuent de mani√®re √©gale au mod√®le. Elle est particuli√®rement utile pour les mod√®les sensibles √† la mise √† l‚Äô√©chelle des fonctionnalit√©s, tels que les algorithmes bas√©s sur la descente en d√©grad√©. Le `MinMaxScaler` fonctionne sur les param√®tres suivants :

- **min** : limite inf√©rieure de la transformation, partag√©e par toutes les fonctionnalit√©s. La valeur par d√©faut est `0.0`.
- **max** : limite sup√©rieure de la transformation, partag√©e par toutes les fonctionnalit√©s. La valeur par d√©faut est `1.0`.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler).  -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : `Array[Double]`
- Type de donn√©es de sortie : `Array[Double]`

**D√©finition**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|--------------------------------------------------------------------------------------------------|------|---------|----------|
| `min` | Limite inf√©rieure apr√®s transformation, partag√©e par toutes les fonctionnalit√©s. | double | 0,0 | facultatif |
| `max` | Limite sup√©rieure apr√®s transformation, partag√©e par toutes les fonctionnalit√©s. | double | 1.0 | facultatif |

**Exemple de transformation**

Cet exemple transforme un ensemble de fonctionnalit√©s, les redimensionnant √† la plage sp√©cifi√©e √† l‚Äôaide de MinMaxScaler apr√®s avoir appliqu√© plusieurs autres transformations.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling, min_max_scaler(maxScaling) as features)
```

#### MaxAbsScaler {#maxabsscaler}

Le transformateur `MaxAbsScaler` redimensionne chaque fonctionnalit√© d‚Äôun jeu de donn√©es de lignes vectorielles sur la plage [-1, 1] en la divisant par la valeur absolue maximale de chaque fonctionnalit√©. Cette transformation est id√©ale pour pr√©server la dispersion des jeux de donn√©es avec des valeurs positives et n√©gatives, car elle ne d√©place ni ne centre les donn√©es. Cela rend le `MaxAbsScaler` particuli√®rement adapt√© aux mod√®les qui sont sensibles √† l‚Äô√©chelle des fonctionnalit√©s d‚Äôentr√©e, comme ceux qui impliquent des calculs de distance.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#maxabsscaler). -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : `Array[Double]`
- Type de donn√©es de sortie : `Array[Double]`

**D√©finition**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|---------------------------------------------------------------------------------------------------------|------|---------|----------|
| NA | MaxAbsScaler ne n√©cessite aucun param√®tre suppl√©mentaire pour son fonctionnement. | NA | NA | NA |

**Exemple de transformation**

Cet exemple applique plusieurs transformations, y compris `MaxAbsScaler`, pour redimensionner les fonctionnalit√©s dans la plage [-1, 1].

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, max_abs_scaler(vec_assembler) as maxScaling)
```

#### Normaliseur {#normalizer}

`Normalizer` est un transformateur qui normalise chaque vecteur dans un jeu de donn√©es de lignes vectorielles pour avoir une norme unitaire. Ce processus assure une √©chelle coh√©rente sans modifier la direction des vecteurs. Cette transformation est particuli√®rement utile dans les mod√®les d&#39;apprentissage automatique qui reposent sur des mesures √† distance ou d&#39;autres calculs vectoriels, surtout lorsque l&#39;ampleur des vecteurs varie consid√©rablement.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#normalizer) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : `array[double]` / `vector[double]`
- Type de donn√©es de sortie : `vector[double]`

**D√©finition**

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|----------------------------------------------------------------------------------------|---------|---------|----------|
| `p` | Sp√©cifie le `p-norm` utilis√© pour la normalisation (par exemple, `1-norm`, `2-norm`, etc.). | Entier | 2 | facultatif |

**Exemple de transformation**

Cet exemple montre comment appliquer plusieurs transformations, y compris `Normalizer`, pour normaliser un ensemble de fonctionnalit√©s √† l‚Äôaide du `p-norm` sp√©cifi√©.

```sql
TRANSFORM(binarizer(time_spent, 5.0) as binary, bucketizer(course_duration, array(-440.5, 0.0, 150.0, 1000.7)) as buck_features, vector_assembler(array(buck_features, users_count, binary)) as vec_assembler, normalizer(vec_assembler, 3) as normalized)
```

#### QuantileDiscretizer {#quantilediscretizer}

`QuantileDiscretizer` est un transformateur qui convertit une colonne avec des fonctions continues en fonctions cat√©goriques binaires, avec le nombre de classes d√©termin√© par le param√®tre `numBuckets` . Dans certains cas, le nombre r√©el de compartiments peut √™tre inf√©rieur √† ce nombre sp√©cifi√© s‚Äôil existe trop peu de valeurs distinctes pour cr√©er suffisamment de quantiles.

Cette transformation est particuli√®rement utile pour simplifier la repr√©sentation des donn√©es continues ou pour les pr√©parer aux algorithmes qui fonctionnent mieux avec l‚Äôentr√©e cat√©gorique.

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : colonne num√©rique
- Type de donn√©es de sortie : colonne num√©rique (cat√©gorie)

**D√©finition**

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|--------------|--------------------------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `NUM_BUCKETS` | Nombre de compartiments (quantiles ou cat√©gories) dans lesquels les points de donn√©es sont regroup√©s. Ce nombre doit √™tre sup√©rieur ou √©gal √† deux. | Entier | 2 | facultatif |

**Exemple de transformation**

Cet exemple montre comment `QuantileDiscretizer` classe une colonne de fonctionnalit√©s continues (`hour`) en trois compartiments cat√©goriques.

```sql
TRANSFORM(quantile_discretizer(hour, 3) as result)
```

**Exemple avant et apr√®s la discr√©tisation**

| identifiant | hour | result |
|---|------|--------|
| 0 | 18,0 | 2.0 |
| 1 | 19,0 | 2.0 |
| 2 | 8,0 | 1.0 |
| 3 | 5.0 | 1.0 |
| 4 | 2,2 | 0,0 |

#### StandardScaler {#standardscaler}

`StandardScaler` est un transformateur qui normalise chaque fonctionnalit√© d‚Äôun jeu de donn√©es de lignes vectorielles pour obtenir une √©cart-type unitaire et/ou une moyenne nulle. Ce processus rend les donn√©es plus adapt√©es aux algorithmes qui supposent que les fonctionnalit√©s sont centr√©es autour de z√©ro avec une √©chelle coh√©rente. Cette transformation est particuli√®rement importante pour les mod√®les d&#39;apprentissage automatique tels que SVM, la r√©gression logistique et les r√©seaux neuronaux, o√π des donn√©es non normalis√©es pourraient entra√Æner des probl√®mes de convergence ou une pr√©cision r√©duite.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#standardscaler).  -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : vector
- Type de donn√©es de sortie : Vector

**D√©finition**

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|------------|------------------------------------------------------------------------------------------------------|---------|---------|----------|
| `withStd` | Met √† l‚Äô√©chelle les donn√©es pour obtenir l‚Äô√©cart type unitaire. | Bool√©en | True | facultatif |
| `withMean` | Centre les donn√©es avec la moyenne avant la mise √† l‚Äô√©chelle. Cette option produit une sortie dense, il faut donc √™tre prudent avec une entr√©e clairsem√©e. | Bool√©en | False | facultatif |

**Exemple de transformation**

Cet exemple montre comment appliquer StandardScaler √† un ensemble de fonctionnalit√©s, en les normalisant avec l‚Äô√©cart-type unitaire et la moyenne nulle.

```sql
TRANSFORM(standard_scaler(feature) as ss_features)
```

### Conversions cat√©gorielles {#categorical-transformations}

Lisez cette section pour un aper√ßu des transformateurs disponibles con√ßus pour convertir et pr√©traiter des donn√©es cat√©goriques pour les mod√®les d‚Äôapprentissage automatique. Ces transformations sont con√ßues pour des points de donn√©es qui repr√©sentent des cat√©gories ou des √©tiquettes distinctes, plut√¥t que des valeurs num√©riques.

#### StringIndexer {#stringindexer}

`StringIndexer` est un transformateur qui code une colonne de cha√Æne d‚Äô√©tiquettes dans une colonne d‚Äôindex num√©riques. Les index sont compris entre 0 et `numLabels` et sont class√©s par fr√©quence d‚Äô√©tiquette (le libell√© le plus courant re√ßoit un index de 0). Si la colonne d‚Äôentr√©e est num√©rique, elle est convertie en cha√Æne avant indexation. Des libell√©s non visibles peuvent √™tre affect√©s √† l‚Äôindex `numLabels` s‚Äôils sont sp√©cifi√©s par l‚Äôutilisateur.

Cette transformation est particuli√®rement utile pour convertir des donn√©es de cha√Æne cat√©gorielles sous forme num√©rique, ce qui la rend adapt√©e aux mod√®les d‚Äôapprentissage automatique qui n√©cessitent une entr√©e num√©rique.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stringindexer) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : cha√Æne
- Type de donn√©es de sortie : num√©rique

**D√©finition**

```sql
TRANSFORM(string_indexer(category) as si_category)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|-------------|------|---------|----------|
| NA | `StringIndexer` ne n√©cessite aucun param√®tre suppl√©mentaire pour son fonctionnement. | NA | NA | NA |

**Exemple de transformation**

Cet exemple montre comment appliquer le `StringIndexer` √† une fonction cat√©gorique, en le convertissant en index num√©rique.

```sql
TRANSFORM(string_indexer(category) as si_category)
```

#### OneHotEncoder {#onehotencoder}

`OneHotEncoder` est un transformateur qui convertit une colonne d‚Äôindex de libell√© en une colonne de vecteurs binaires √©pars, o√π chaque vecteur poss√®de au plus une seule valeur. Cet encodage est particuli√®rement utile pour permettre aux algorithmes qui n√©cessitent une entr√©e num√©rique, comme la R√©gression logistique, d‚Äôincorporer efficacement les donn√©es cat√©gorielles.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#onehotencoder).  -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : num√©rique
- Type de donn√©es de sortie : Vector[Int]

**D√©finition**

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|-------------|------|---------|----------|
| NA | OneHotEncoder ne n√©cessite aucun param√®tre suppl√©mentaire pour son fonctionnement. | NA | NA | NA |

**Exemple de transformation**

Cet exemple montre comment appliquer d‚Äôabord le `StringIndexer` √† une fonction cat√©gorique, puis utiliser le `OneHotEncoder` pour convertir les valeurs index√©es en vecteur binaire.

```sql
TRANSFORM(string_indexer(category) as si_category, one_hot_encoder(si_category) as ohe_category)
```

### Transformations textuelles {#textual-transformations}

Cette section fournit des d√©tails sur les transformateurs disponibles pour le traitement et la conversion de donn√©es texte en formats pouvant √™tre utilis√©s par les mod√®les d‚Äôapprentissage automatique. Cette section est essentielle pour les d√©veloppeurs qui travaillent sur les donn√©es de langage naturel et l‚Äôanalyse de texte.

#### CountVectorizer {#countvectorizer}

`CountVectorizer` est un transformateur qui convertit une collection de documents texte en vecteurs de nombre de jetons, produisant des repr√©sentations √©parses bas√©es sur le vocabulaire extrait du corpus. Cette transformation est essentielle pour convertir des donn√©es texte dans un format num√©rique utilisable par des algorithmes d‚Äôapprentissage automatique, tels que LDA (Latent Dirichlet Attribution), en repr√©sentant la fr√©quence des jetons dans chaque document.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#countvectorizer). -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : Array[String]
- Type de donn√©es de sortie : Vecteur Dense

**D√©finition**

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|---------|----------|
| `VOCAB_SIZE` | Taille maximale du vocabulaire. CountVectorizer cr√©e un vocabulaire qui ne prend en compte que les `vocabSize` principaux termes ordonn√©s par fr√©quence de terme dans le corpus. | Int | 218 | facultatif |
| `MIN_DOC_FREQ` | Indique le nombre minimum de documents dans lesquels un terme doit appara√Ætre pour √™tre inclus dans le vocabulaire. Il peut s‚Äôagir d‚Äôun nombre absolu ou d‚Äôune fraction de documents (si le nombre est double). | Double | 1.0 | facultatif |
| `MAX_DOC_FREQ` | Indique le nombre maximal de documents dans lesquels un terme peut appara√Ætre pour √™tre inclus dans le vocabulaire. Il peut s‚Äôagir d‚Äôun nombre absolu ou d‚Äôune fraction de documents (si le nombre est double). | Double | (263)-1 | facultatif |
| `MIN_TERM_FREQ` | Filtre les mots rares d‚Äôun document. Les termes dont la fr√©quence/le nombre est inf√©rieur au seuil donn√© sont ignor√©s. Il peut s‚Äôagir d‚Äôun nombre absolu ou d‚Äôune fraction du nombre de jetons du document. | Double | 1.0 | facultatif |

{style="table-layout:auto"}

**Exemple de transformation**

Cet exemple illustre la mani√®re dont CountVectorizer convertit une collection de tableaux de texte en vecteurs de nombres de jetons, produisant ainsi une repr√©sentation √©parse.

```sql
TRANSFORM(count_vectorizer(texts) as cv_output)
```

**Exemple avant et apr√®s la vectorisation**

| identifiant | text | cv_output |
|----|---------------------------------|-----------------------------------|
| 0 | Array(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) | (3,[0,1,2],[1.0,1.0,1.0]) |
| 1 | Array(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;c&quot;, &quot;a&quot;) | (3,[0,1,2],[2.0,2.0,1.0]) |

{style="table-layout:auto"}

#### NGram {#ngram}

`NGram` est un transformateur qui g√©n√®re une s√©quence d‚Äôn-grammes, o√π un n-gramme est une s√©quence de jetons (&#39;??&#39;) (g√©n√©ralement des mots) pour certains entiers (`ùëõ`). La sortie est compos√©e de cha√Ænes &quot;??&quot; d√©limit√©es par des espaces, qui peuvent √™tre utilis√©es comme fonctionnalit√©s dans les mod√®les d‚Äôapprentissage automatique, en particulier celles ax√©es sur le traitement du langage naturel.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#n-gram). -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : Array[String]
- Type de donn√©es de sortie : Tableau[Cha√Æne]

**D√©finition**

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|-----------------------------------------------------------------------------------------------|---------|-------------------|----------|
| `N` | La longueur minimale de n-gramme doit √™tre sup√©rieure ou √©gale √† 1. | Entier | 2 (fonctions de bigramme) | facultatif |

{style="table-layout:auto"}

**Exemple de transformation**

Cet exemple montre comment le transformateur NGram cr√©e une s√©quence de 3 grammes √† partir d&#39;une liste de jetons d√©riv√©s de donn√©es textuelles.

```sql
TRANSFORM(tokenizer(review_comments) as token_comments, ngram(token_comments, 3) as n_tokens)
```

**Exemple avant et apr√®s la transformation n-gramme**

| identifiant | text | n_tokens |
|----|-------------------------------------------------------|-------------------------------------------------------|
| 0 | [&quot;this&quot;, &quot;was&quot;, &quot;an&quot;, &quot;amusant&quot;, &quot;movie&quot;] | [&quot;c&#39;√©tait un&quot;, &quot;c&#39;√©tait un film divertissant&quot;, &quot;un film divertissant&quot;] |

{style="table-layout:auto"}

#### StopWordsRemover {#stopwordsremover}

`StopWordsRemover` est un transformateur qui supprime les mots d‚Äôarr√™t d‚Äôune s√©quence de cha√Ænes, en √©liminant les mots courants qui n‚Äôont pas de signification significative. Elle prend en entr√©e une s√©quence de cha√Ænes (comme la sortie d‚Äôun jeton) et supprime tous les mots-cl√©s sp√©cifi√©s par le param√®tre `stopWords`.

Cette transformation est utile pour le pr√©traitement des donn√©es texte, ce qui am√©liore l‚Äôefficacit√© des mod√®les d‚Äôapprentissage automatique en aval en √©liminant les mots qui ne contribuent pas beaucoup √† la signification globale.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#stopwordsremover) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : Array[String]
- Type de donn√©es de sortie : Tableau[Cha√Æne]

**D√©finition**

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|--------------------|--------------------------------------------------------------------------------------------------|---------------|-------------------------|----------|
| `stopWords` | Les mots √† filtrer. | array [string] | Valeur par d√©faut : mots-cl√©s anglais | facultatif |

{style="table-layout:auto"}

<!-- Q) should this be the `CUSTOM_STOP_WORDS` parameter or the `stopWords` parameter?  -->

**Exemple de transformation**

Cet exemple montre comment le `StopWordsRemover` filtre les mots-cl√©s anglais courants d‚Äôune liste de jetons.

```sql
TRANSFORM(stop_words_remover(raw) as filtered)
```

**Exemple avant et apr√®s la suppression de mots vides**

| identifiant | raw | filtr√© |
|----|------------------------------|--------------------------|
| 0 | [J&#39;ai vu, vu, rouge, ballon] | [scie, rouge, ballon] |
| 1 | [Mary, had, a, small, agneau] | [Marie, petite, agneau] |

**Exemple avec mots d‚Äôarr√™t personnalis√©s**

Cet exemple illustre l‚Äôutilisation d‚Äôune liste personnalis√©e de mots d‚Äôarr√™t pour filtrer des mots sp√©cifiques des s√©quences d‚Äôentr√©e.

```sql
TRANSFORM(stop_words_remover(raw, array("red", "I", "had")) as filtered)
```

**Exemple avant et apr√®s suppression de mots d‚Äôarr√™t personnalis√©s**

| identifiant | raw | filtr√© |
|----|------------------------------|--------------------------|
| 0 | [J&#39;ai vu, vu, rouge, ballon] | [vu, le, le ballon] |
| 1 | [Mary, had, a, small, agneau] | [Mary, a, small, agneau] |

#### TF-IDF {#tf-idf}

`TF-IDF` (Term Frequency-Inverse Document Frequency) est un transformateur utilis√© pour mesurer l‚Äôimportance d‚Äôun mot dans un document par rapport √† un corpus. Fr√©quence des termes (TF) fait r√©f√©rence au nombre de fois o√π un terme \(t\) appara√Æt dans un document \(d\), tandis que la fr√©quence des documents (DF) mesure le nombre de documents dans le corpus \(D\) contenant le terme \(t\). Cette m√©thode est largement utilis√©e dans l‚Äôextraction de texte pour r√©duire l‚Äôinfluence des mots courants, tels que &quot;a&quot;, &quot;the&quot; et &quot;of&quot;, qui contiennent peu d‚Äôinformations uniques.

Cette transformation s‚Äôav√®re particuli√®rement utile dans les t√¢ches d‚Äôextraction de texte et de traitement du langage naturel, dans la mesure o√π elle attribue une valeur num√©rique √† l‚Äôimportance de chaque mot dans un document et dans l‚Äôensemble du corpus.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : Array[String]
- Type de donn√©es de sortie : Vector[Int]

**D√©finition**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------------|----------------------------------------------------------------------------------------|------|---------|----------|
| `NUM_FEATURES` | Nombre de fonctionnalit√©s √† g√©n√©rer. Doit √™tre sup√©rieur √† 0. | Int | 262144 | facultatif |
| `MIN_DOC_FREQ` | Nombre minimum de documents dans lesquels un terme doit appara√Ætre inclus dans le mod√®le. | Int | 0 | facultatif |

{style="table-layout:auto"}

**Exemple de transformation**

Cet exemple montre comment utiliser TF-IDF pour transformer des phrases en jetons en un vecteur de fonctionnalit√©s qui repr√©sente l‚Äôimportance de chaque terme dans le contexte du corpus entier.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Tokenizer {#tokenizer}

`Tokenizer` est un transformateur qui ventile le texte, tel qu‚Äôune phrase, en termes individuels, g√©n√©ralement en mots. Il convertit les phrases en tableaux de jetons, ce qui fournit une √©tape fondamentale du pr√©traitement de texte qui pr√©pare les donn√©es pour d‚Äôautres processus d‚Äôanalyse de texte ou de mod√©lisation.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tokenizer) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : phrase textuelle
- Type de donn√©es de sortie : Tableau[Cha√Æne]

**D√©finition**

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|-----------|-------------|------|---------|----------|
| NA | `Tokenizer` ne n√©cessite aucun param√®tre suppl√©mentaire pour son fonctionnement. | NA | NA | NA |

**Exemple de transformation**

Cet exemple montre comment `Tokenizer` ventile des phrases en mots individuels (jetons) dans le cadre d‚Äôun pipeline de traitement de texte.

```sql
create table td_idf_model transform(tokenizer(sentence) as token_sentence, tf_idf(token_sentence) as tf_sentence, vector_assembler(array(tf_sentence)) as feature) OPTIONS()
```

#### Word2Vec {#word2vec}

`Word2Vec` est un estimateur qui traite des s√©quences de mots repr√©sentant des documents et forme un `Word2VecModel`. Ce mod√®le associe chaque mot √† un vecteur de taille fixe unique et convertit chaque document en vecteur en calculant la moyenne des vecteurs de tous les mots du document. Largement utilis√©e dans les t√¢ches de traitement de langage naturel, `Word2Vec` cr√©e des int√©grations de mots qui capturent la signification s√©mantique, convertissant les donn√©es de texte en vecteurs num√©riques qui repr√©sentent les relations entre les mots et permettant une analyse de texte plus efficace et des mod√®les d‚Äôapprentissage automatique.

<!-- More information and examples can be found in the [Spark algorithm documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#word2vec) -->

**Types des donn√©es**

- Type de donn√©es d‚Äôentr√©e : Array[String]
- Type de donn√©es de sortie : Vector[Double]

**D√©finition**

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Param√®tres**

| Param√®tre | Description | Type | Par d√©faut | Facultatif |
|--------------|-----------------------------------------------------------------------------------------------------|---------|---------|----------|
| `VECTOR_SIZE` | Dimension du vecteur dans lequel chaque mot est transform√©. | Nombre entier | 100 | facultatif |
| `MIN_COUNT` | Nombre minimum de fois o√π un jeton doit appara√Ætre inclus dans le vocabulaire du mod√®le `Word2Vec`. | Nombre entier | 5 | facultatif |

{style="table-layout:auto"}

**Exemple de transformation**

Cet exemple montre comment `Word2Vec` convertit une r√©vision en unit√©s lexicales en vecteur de taille fixe repr√©sentant la moyenne des vecteurs de mots dans le document.

```sql
TRANSFORM(tokenizer(review) as tokenized, word2Vec(tokenized, 10, 1) as word2Vec)
```

**Exemple avant et apr√®s la transformation Word2Vec**

| review | tokenized | word2Vec |
|-------------------------------|--------------------------------------|---------------------------------|
| c&#39;√©tait un film divertissant. | [ceci, √©tait, un, divertissant, film] | [-0.025713888928294182,0.00818799751577899,0.0092235435731709,-0.01515385233797133,0.012175946310162545,3.1129065901041035E-4,0.0025145105042611252,0.005757019785232843,-0.021328244300093502,0.009335877187550069{1] |

{style="table-layout:auto"}
