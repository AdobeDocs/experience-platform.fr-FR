---
keywords: Experience Platform;accueil;rubriques les plus consultées;Amazon S3;amazon s3;S3;s3
solution: Experience Platform
title: Présentation du connecteur source Amazon S3
topic-legacy: overview
description: Découvrez comment connecter S3 à Adobe Experience Platform à l’aide d’API ou de l’interface utilisateur.
exl-id: e38c7d09-5f5e-4c8a-b494-dce9f984f3d8
source-git-commit: 32f0f59a232b76b3af0bb921c165275d78185a59
workflow-type: tm+mt
source-wordcount: '618'
ht-degree: 49%

---

# Connecteur Amazon S3

Adobe Experience Platform fournit une connectivité native pour les fournisseurs cloud tels qu’AWS, [!DNL Google Cloud Platform], et [!DNL Azure]. Vous pouvez importer vos données de ces systèmes dans [!DNL Platform].

Les sources de stockage dans le cloud peuvent introduire vos propres données dans [!DNL Platform] sans avoir à les télécharger, les formater ou les transférer. Les données ingérées peuvent être formatées sous la forme XDM JSON, XDM Parquet ou délimitées. Chaque étape du processus est intégrée dans le processus Sources. [!DNL Platform] vous permet d’importer des données de S3 par lots.

## Liste autorisée d’adresses IP

Une liste d’adresses IP doit être ajoutée à une liste autorisée avant d’utiliser les connecteurs source. Si vous n’ajoutez pas vos adresses IP spécifiques à une région à votre liste autorisée, des erreurs ou une absence de performances peuvent se produire lors de l’utilisation de sources. Voir la page [liste autorisée d’adresses IP](../../ip-address-allow-list.md) pour plus d’informations.

## Contraintes de dénomination pour fichiers et répertoires

La liste suivante inclut les contraintes dont vous devez tenir compte lorsque vous nommez votre fichier ou répertoire de stockage dans le cloud.

- Les noms des composants de répertoire et de fichier ne doivent pas dépasser 255 caractères.
- Les noms de répertoire et de fichier ne peuvent pas se terminer par une barre oblique (`/`). Elle sera le cas échéant automatiquement supprimée.
- Les caractères d’URL réservés suivants doivent être des caractères d’échappement : `! ' ( ) ; @ & = + $ , % # [ ]`
- Les caractères suivants ne sont pas autorisés : `" \ / : | < > * ?`.
- Caractères de chemin d’URL illégaux interdits. Les points de code tels que `\uE000`, bien que valides dans les noms de fichier NTFS, ne sont pas des caractères Unicode valides. En outre, certains caractères ASCII ou Unicode, tels que les caractères de contrôle (0x00 à 0x1F, \u0081, etc.), ne sont pas non plus autorisés. Pour les règles régissant les chaînes Unicode en HTTP/1.1, voir [RFC 2616, section 2.2 : règles de base](https://www.ietf.org/rfc/rfc2616.txt) et [RFC 3987](https://www.ietf.org/rfc/rfc3987.txt).
- Les noms de fichier suivants ne sont pas autorisés : LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, LPT9, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, PRN, AUX, NUL, CON, CLOCK$, point (.) et deux points (..).

## Conditions préalables {#prerequisites}

Pour ingérer un seul répertoire avec S3, vous devez créer une [!DNL Identity and Access Management] (IAM) pour Platform dans la console S3 et attribuez des autorisations pour les actions suivantes :

- `s3:GetObject`
- `s3:GetObjectVersion`

Les autorisations suivantes sont également requises pour explorer et tester la connectivité :

- `s3:ListAllMyBuckets`
- `s3:ListBucket`
- `s3:GetBucketLocation`

Un chemin de fichier tel que `myBucket/folder/subfolder/subsubfolder/abc.csv` ne peut vous conduire qu’à accéder à `subsubfolder/abc.csv`. Si vous souhaitez accéder au sous-dossier, vous pouvez spécifier la variable `bucket` dans votre console S3 en tant que `myBucket` et le `folderPath` as `folder/subfolder` pour vous assurer que l’exploration des fichiers commence à `subfolder` par opposition à `subsubfolder/abc.csv`.

## Utilisation d’informations d’identification de sécurité temporaires pour la connexion [!DNL Amazon S3]

Vous pouvez vous connecter. [!DNL Amazon S3] avec des informations d’identification de sécurité temporaires à l’aide de la fonction `s3SessionToken`. Cela vous permet de vous connecter. [!DNL Amazon S3] vers Platform sans avoir à créer des informations d’identification IAM permanentes avec [!DNL Amazon Web Services]ou vous donner accès à votre [!DNL Amazon S3] pour les utilisateurs dans des environnements non approuvés.

Les informations d’identification de sécurité temporaires fonctionnent de la même manière que les informations d’identification clés d’accès à long terme standard, sauf que vous pouvez configurer une date d’expiration plus courte pour vos informations d’identification temporaires. Les expirations peuvent être définies sur quelques minutes après l’activation ou jusqu’à plusieurs heures. Les informations d’identification temporaires ne sont pas non plus contenues avec l’utilisateur. Cela signifie que vous devez demander un nouvel ensemble d’informations d’identification temporaires, lorsqu’elles expirent.

Pour obtenir des instructions sur la génération de votre jeton de session temporaire, reportez-vous à cette section [[!DNL AWS] document sur les jetons de session temporaire](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#api_getsessiontoken)
.

## Connexion de S3 à [!DNL Platform]

La documentation ci-dessous fournit des informations sur la connexion de S3 à Adobe Experience Platform à l’aide des API ou de l’interface utilisateur :

### Utiliser les API

- [Création d’une connexion de base S3 à l’aide de l’API Flow Service](../../tutorials/api/create/cloud-storage/s3.md)
- [Explorer la structure de données et le contenu d’une source de stockage dans le cloud à l’aide de l’API Flow Service](../../tutorials/api/explore/cloud-storage.md)
- [Créer un flux de données pour une source de stockage dans le cloud à l’aide de l’API Flow Service](../../tutorials/api/collect/cloud-storage.md)

### Utiliser l’interface utilisateur

- [Création d’une connexion source Amazon S3 dans l’interface utilisateur](../../tutorials/ui/create/cloud-storage/s3.md)
- [Créer un flux de données pour une connexion de stockage dans le cloud dans l’interface utilisateur](../../tutorials/ui/dataflow/batch/cloud-storage.md)
